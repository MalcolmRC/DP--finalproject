The central question the following project set out to answer was: is there a mismatch between police patrol allocation and crime in the city of Medellin, COL? Given a rich dataset, including geocoded and time stamped crime data, as well as demographic and socioeconomic indicators at the block and quadrant level, we were well-positioned to attempt to answer this question. It is an important issue to consider since police are a scarce resource and one of the only tools available to the state to combat and reduce crime. Therefore, it is of the utmost importance for this resource, police patrols, to be distributed efficiently across the city in order to prevent and combat crime.

We will discuss our approach in the following order. First, we will lay out what the raw data we received looked like and outline the steps we took to "clean" it to make it usable for our purposes. Then, we will describe how we used the clean data to visualize crime and police patrol distribution across Medellin. Next, we will detail our use of text processing to analyze two Brooking articles describing security issues both in Medellin and in Colombia as a whole. Finally, we will fit a regression model with lasso feature selection to predict violent crime per quadrant in Medellin using area-specific demographic and socioeconomic data.

### 1. Data wrangling

The raw data we received was directly from the National Police of Colombia. Rather than have everything well-organized within one dataset, they sent us several data sets, with overlapping information, but each containing a useful set of variables. One of them contained geocoded crime data at the street block level. Another contained geocoded crime data at the quadrant level (akin to police beats in the US). Another dataset contained the labels for the strangely coded variable names, to facilitate interpretation. We proceeded to clean each of these data sets and prepare them for merging, so that we could have all the information in a single place. 

The cleaning process involved several steps, which we will briefly glance over. Having chosen the quadrant as the unit of analysis of interest, we created a variable in the block level data that identified which quadrant each street block belonged to given its geocoded location. We also averaged 2018-2021 data to get the yearly mean and facilitate plotting. Furthermore, using crimes' time stamps, we identified during which police shifts crimes were committed, to analyze the mismatch between police distribution and crimes across all three daily police shifts. Finally, having made the data tidy, using the crime variable and knowing that officers were deployed uniformly across quadrants, we calculated a crimes per officer variable that would be central to measuring the success of our re-distribution proposal.

To wrap up the data wrangling, we put forth a police patrol re-distribution recommendation that would assign officers to quadrants proportionally to the crime levels, rather than uniformly so. The purpose of this was to visualize in the next step whether crime would be better combated if there was a better strategy for police patrol distribution than the status quo.

### 2. Plotting

With clean data, the plotting was merely about reproducing reality in maps of Medellin. We started by showing how crime and police patrols were currently distributed across the city. We plotted this for each of the three police shifts (morning, afternoon and night). We then went on to plot how police officers would be re-distributed according to our proposal and subsequently, how crimes per officers would change across quadrants if crime remained constant and officers were re-allocated as we proposed. The maps show that our recommendation has potential to improve the status quo in terms of homogenizing officer workload and reducing average crimes per officer across the board.

To make the visualization of these plots more interactive, we created a Shiny interface. This would allow anyone interested in viewing the results of our work to do so in an accessible and approachable manner. The Shiny interface allows the viewer to choose what he wants to visualize, and the options are between two different types of maps: one that shows the police patrol distribution and the other that shows crime distribution. Shiny allows the viewer to pick which police shift to visualize and whether to see the status quo or the hypothetical state of affairs after re-distribution.

### 3. Text Analysis

Text analysis served as a means to provide some theoretical substance to our data. We analyzed two articles by Vanda Felbab-Brown, a recognized authority on security at the Brookings Institution. In the first article, she analyzes the security challenges faced by Medellin, discussing the effectiveness of policy solutions, including policing, deployed to quell urban violence. In the second, she takes a broader perspective, discussing former president Juan Manuel Santos' national security plan to address political and urban violence. Both articles highlight, among other things, the importance of strategic deployment of policing resources as a means to combat crime and violence. 

The text analysis consisted of four parts: sentiment analysis, word frequency, cooccurrence analysis, and word relations. The first step was to extract the article text from the Brookings website. To do so, we created a function to simultaneously scrape the website and format the textst. We found that the most common words and lemmas in the Medellin article differed from those in the article about national security. While the top words in the former included urban violence, crime, police and communities, the latter focused on the macro level, with national security policy, government and the economy among the top words. Sentiment analysis with and without negation showed that the Medellin article was more pessimistic that the national article. Finally, cooccurrence and word relations provided additional insights toward understanding the general "feel" of the articles. We focused on the words violence, security and police, using correlations and sentences to understand what was said about each. 


### 4. Fitting a Model

In the first stage of the modelling component of the assignment, we fit an ordinary least squares regression to explore the relationship between the socioeconomic, demographic and land-use data and the sum of crimes per quadrant. Prior to fitting the model, we performed some basic data exploration and feature engineering. We dropped variables without variance, converted character variables to factors, and logged the outcome to counter make the distribution more normal and counter its right skew. The ordinary least squares regression produced many significant predictors, but also revealed many irrelevant variables.

With this in mind, we decided to use the lasso regularization to select predictive variables and avoid overfitting. We removed NAs, converted our data into a model matrix, and performed 10-fold cross validation to select predictive variables. The model with the lowest cross validation mean squared error included 46 variables, and the simplest model with an loss within 1 standard error of the minimum selected 33 variables. To test the predictive performance of the variables selected by the lasso, we split the data into a training set and a test set, and predicted the log sum of crimes using both the minimum error model and the 1 standard error model. Both predicted crime per quadrant well, with the simpler model having the lower RMSE.


### Conclusion

Policing is a critical element of any public security platform. We find that the inefficient distribution of police patrols in Medellin may contribute to suboptimal safety outcomes across the city, particularly in the neighborhoods that need it most. In this exercise, we took messy raw data and tidied it to visualize patterns and relationships, predict crimes per quadrant, and propose alternative allocations of police patrols to improve public safety outcomes. Text analysis allowed us to gauge the sentiment of a leading scholar vis-a-vis security and policing in Medellin and Colombia, providing a theoretical backdrop to bolster our findings. This is an admittedly simple study, but it provides some motivation for a more profound analysis, and presents evidence that data and programming are critical in tackling critical policy challenges.      
